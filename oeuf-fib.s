# File generated by CompCert 2.6
# Command line: -c -dasm
	.text
	.align	16
	.globl _$1
_$1:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %edx
	movl	%edx, 0(%esp)
	movl	0(%edx), %eax
	movl	4(%eax), %eax
	addl	$12, %esp
	ret
	.cfi_endproc
	.type	_$1, @function
	.size	_$1, . - _$1
	.text
	.align	16
	.globl _$2
_$2:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %ebx
	movl	$12, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	leal	_$1, %ecx
	movl	%ecx, 0(%eax)
	movl	%ebx, 4(%eax)
	movl	4(%esi), %edx
	movl	%edx, 8(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$2, @function
	.size	_$2, . - _$2
	.text
	.align	16
	.globl _$3
_$3:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %edx
	movl	%edx, 0(%esp)
	movl	4(%edx), %eax
	addl	$12, %esp
	ret
	.cfi_endproc
	.type	_$3, @function
	.size	_$3, . - _$3
	.text
	.align	16
	.globl _$4
_$4:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %edx
	movl	%edx, 8(%esp)
	movl	%ebx, 12(%esp)
	movl	%esi, 16(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	$8, %eax
	movl	%eax, 0(%esp)
	call	malloc
	movl	$1, %edx
	movl	%edx, 0(%eax)
	movl	%esi, 4(%eax)
	movl	4(%ebx), %ecx
	movl	0(%ecx), %ebx
	movl	%ecx, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%ebx
	movl	12(%esp), %ebx
	movl	16(%esp), %esi
	addl	$28, %esp
	ret
	.cfi_endproc
	.type	_$4, @function
	.size	_$4, . - _$4
	.text
	.align	16
	.globl _$5
_$5:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	$32, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	leal	_$4, %ecx
	movl	%ecx, 0(%eax)
	movl	%esi, 4(%eax)
	movl	4(%ebx), %edx
	movl	%edx, 8(%eax)
	movl	8(%ebx), %ecx
	movl	%ecx, 12(%eax)
	movl	12(%ebx), %ecx
	movl	%ecx, 16(%eax)
	movl	16(%ebx), %edx
	movl	%edx, 20(%eax)
	movl	20(%ebx), %ecx
	movl	%ecx, 24(%eax)
	movl	24(%ebx), %edx
	movl	%edx, 28(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$5, @function
	.size	_$5, . - _$5
	.text
	.align	16
	.globl _$6
_$6:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	$28, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$5, %ecx
	movl	%ecx, 0(%eax)
	movl	%esi, 4(%eax)
	movl	4(%ebx), %ecx
	movl	%ecx, 8(%eax)
	movl	8(%ebx), %ecx
	movl	%ecx, 12(%eax)
	movl	12(%ebx), %edx
	movl	%edx, 16(%eax)
	movl	16(%ebx), %ecx
	movl	%ecx, 20(%eax)
	movl	20(%ebx), %ecx
	movl	%ecx, 24(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$6, @function
	.size	_$6, . - _$6
	.text
	.align	16
	.globl _$7
_$7:
	.cfi_startproc
	subl	$44, %esp
	.cfi_adjust_cfa_offset	44
	leal	48(%esp), %edx
	movl	%edx, 8(%esp)
	movl	%ebx, 12(%esp)
	movl	%esi, 16(%esp)
	movl	%edi, 20(%esp)
	movl	%ebp, 24(%esp)
	movl	0(%edx), %edi
	movl	4(%edx), %esi
	movl	8(%edi), %ecx
	movl	0(%ecx), %eax
	movl	%ecx, 0(%esp)
	movl	%esi, 4(%esp)
	call	*%eax
	movl	%eax, 32(%esp)
	movl	$24, %edx
	movl	%edx, 0(%esp)
	call	malloc
	movl	%eax, %ebp
	leal	_$6, %eax
	movl	%eax, 0(%ebp)
	movl	%esi, 4(%ebp)
	movl	4(%edi), %ecx
	movl	%ecx, 8(%ebp)
	movl	8(%edi), %edx
	movl	%edx, 12(%ebp)
	movl	12(%edi), %ecx
	movl	%ecx, 16(%ebp)
	movl	16(%edi), %ecx
	movl	%ecx, 20(%ebp)
	movl	$24, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	movl	%eax, %ebx
	leal	_$3, %eax
	movl	%eax, 0(%ebx)
	movl	%esi, 4(%ebx)
	movl	4(%edi), %eax
	movl	%eax, 8(%ebx)
	movl	8(%edi), %ecx
	movl	%ecx, 12(%ebx)
	movl	12(%edi), %ecx
	movl	%ecx, 16(%ebx)
	movl	16(%edi), %edx
	movl	%edx, 20(%ebx)
	movl	$12, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$12, %edx
	movl	%edx, 0(%eax)
	movl	%ebp, 4(%eax)
	movl	%ebx, 8(%eax)
	movl	0(%eax), %ebx
	movl	4(%edi), %ecx
	movl	%eax, 0(%esp)
	movl	%ecx, 4(%esp)
	call	*%ebx
	movl	0(%eax), %edx
	movl	%eax, 0(%esp)
	movl	%esi, 4(%esp)
	call	*%edx
	movl	32(%esp), %ecx
	movl	0(%ecx), %edx
	movl	32(%esp), %ecx
	movl	%ecx, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%edx
	movl	12(%esp), %ebx
	movl	16(%esp), %esi
	movl	20(%esp), %edi
	movl	24(%esp), %ebp
	addl	$44, %esp
	ret
	.cfi_endproc
	.type	_$7, @function
	.size	_$7, . - _$7
	.text
	.align	16
	.globl _$8
_$8:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %ebx
	movl	$20, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$7, %edx
	movl	%edx, 0(%eax)
	movl	%ebx, 4(%eax)
	movl	4(%esi), %ecx
	movl	%ecx, 8(%eax)
	movl	8(%esi), %edx
	movl	%edx, 12(%eax)
	movl	12(%esi), %ecx
	movl	%ecx, 16(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$8, @function
	.size	_$8, . - _$8
	.text
	.align	16
	.globl _$9
_$9:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	$16, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	leal	_$8, %ecx
	movl	%ecx, 0(%eax)
	movl	%esi, 4(%eax)
	movl	4(%ebx), %ecx
	movl	%ecx, 8(%eax)
	movl	8(%ebx), %edx
	movl	%edx, 12(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$9, @function
	.size	_$9, . - _$9
	.text
	.align	16
	.globl _$10
_$10:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 12(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %ebx
	movl	$12, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	leal	_$9, %ecx
	movl	%ecx, 0(%eax)
	movl	%ebx, 4(%eax)
	movl	4(%esi), %edx
	movl	%edx, 8(%eax)
	movl	8(%esp), %ebx
	movl	12(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	_$10, @function
	.size	_$10, . - _$10
	.text
	.align	16
	.globl _$11
_$11:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %edx
	movl	%edx, 8(%esp)
	movl	%ebx, 12(%esp)
	movl	%esi, 16(%esp)
	movl	%edi, 20(%esp)
	movl	4(%edx), %edi
	movl	$8, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	movl	%eax, %ebx
	leal	_$10, %edx
	movl	%edx, 0(%ebx)
	movl	%edi, 4(%ebx)
	movl	$8, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	movl	%eax, %esi
	leal	_$2, %edx
	movl	%edx, 0(%esi)
	movl	%edi, 4(%esi)
	movl	$12, %edx
	movl	%edx, 0(%esp)
	call	malloc
	leal	_$13, %ecx
	movl	%ecx, 0(%eax)
	movl	%ebx, 4(%eax)
	movl	%esi, 8(%eax)
	movl	0(%eax), %edx
	movl	%eax, 0(%esp)
	movl	%edi, 4(%esp)
	call	*%edx
	movl	%eax, %ebx
	movl	$4, %edx
	movl	%edx, 0(%esp)
	call	malloc
	xorl	%edx, %edx
	movl	%edx, 0(%eax)
	movl	0(%ebx), %ecx
	movl	%ebx, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%ecx
	movl	%eax, %esi
	movl	$4, %ecx
	movl	%ecx, 0(%esp)
	call	malloc
	movl	%eax, %ebx
	xorl	%eax, %eax
	movl	%eax, 0(%ebx)
	movl	$8, %eax
	movl	%eax, 0(%esp)
	call	malloc
	movl	$1, %edx
	movl	%edx, 0(%eax)
	movl	%ebx, 4(%eax)
	movl	0(%esi), %ecx
	movl	%esi, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%ecx
	movl	12(%esp), %ebx
	movl	16(%esp), %esi
	movl	20(%esp), %edi
	addl	$28, %esp
	ret
	.cfi_endproc
	.type	_$11, @function
	.size	_$11, . - _$11
	.text
	.align	16
	.globl _$12
_$12:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %edx
	movl	%edx, 8(%esp)
	movl	%ebx, 12(%esp)
	movl	%esi, 16(%esp)
	movl	%edi, 20(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	0(%esi), %ecx
	testl	%ecx, %ecx
	je	.L100
	cmpl	$1, %ecx
	jne	.L101
	movl	4(%ebx), %eax
	movl	0(%eax), %edx
	movl	4(%esi), %ecx
	movl	%eax, 0(%esp)
	movl	%ecx, 4(%esp)
	call	*%edx
	movl	%eax, %edi
	movl	$12, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$12, %ecx
	movl	%ecx, 0(%eax)
	movl	4(%ebx), %edx
	movl	%edx, 4(%eax)
	movl	8(%ebx), %edx
	movl	%edx, 8(%eax)
	movl	0(%eax), %ebx
	movl	4(%esi), %edx
	movl	%eax, 0(%esp)
	movl	%edx, 4(%esp)
	call	*%ebx
	movl	0(%edi), %edx
	movl	%edi, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%edx
	jmp	.L101
.L100:
	movl	8(%ebx), %eax
.L101:
	movl	12(%esp), %ebx
	movl	16(%esp), %esi
	movl	20(%esp), %edi
	addl	$28, %esp
	ret
	.cfi_endproc
	.type	_$12, @function
	.size	_$12, . - _$12
	.text
	.align	16
	.globl _$13
_$13:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %edx
	movl	%edx, 8(%esp)
	movl	%ebx, 12(%esp)
	movl	%esi, 16(%esp)
	movl	%edi, 20(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	0(%esi), %ecx
	testl	%ecx, %ecx
	je	.L102
	cmpl	$1, %ecx
	jne	.L103
	movl	4(%ebx), %eax
	movl	0(%eax), %edx
	movl	4(%esi), %ecx
	movl	%eax, 0(%esp)
	movl	%ecx, 4(%esp)
	call	*%edx
	movl	%eax, %edi
	movl	$12, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$13, %ecx
	movl	%ecx, 0(%eax)
	movl	4(%ebx), %edx
	movl	%edx, 4(%eax)
	movl	8(%ebx), %edx
	movl	%edx, 8(%eax)
	movl	0(%eax), %ebx
	movl	4(%esi), %edx
	movl	%eax, 0(%esp)
	movl	%edx, 4(%esp)
	call	*%ebx
	movl	0(%edi), %edx
	movl	%edi, 0(%esp)
	movl	%eax, 4(%esp)
	call	*%edx
	jmp	.L103
.L102:
	movl	8(%ebx), %eax
.L103:
	movl	12(%esp), %ebx
	movl	16(%esp), %esi
	movl	20(%esp), %edi
	addl	$28, %esp
	ret
	.cfi_endproc
	.type	_$13, @function
	.size	_$13, . - _$13
	.text
	.align	16
	.globl _$14
_$14:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %edx
	movl	%edx, 4(%esp)
	movl	$4, %eax
	movl	%eax, 0(%esp)
	call	malloc
	leal	_$11, %ecx
	movl	%ecx, 0(%eax)
	addl	$12, %esp
	ret
	.cfi_endproc
	.type	_$14, @function
	.size	_$14, . - _$14
